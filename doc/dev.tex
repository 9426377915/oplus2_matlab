\documentclass[12pt]{article}

\usepackage[colorlinks]{hyperref}

\topmargin 0in
\textheight 8.75in
\textwidth  5.75in

\parskip 5pt


\usepackage{algorithm}
\usepackage{algorithmic}

%\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newenvironment{alg}[2]
{
\vspace{-0.5cm}
\begin{flushleft}
\begin{minipage}[c]{0.8\linewidth}
\begin{algorithm}[H]
\caption{#1}
\label{#2}
\begin{algorithmic}
}
{
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{flushleft}
}



\begin{document}

\title{OP2 Developer's Guide}
\author{Mike Giles}


\maketitle

\begin{abstract}
This document explains some of the algorithms and implementation
details inside OP2.  It is intended primarily for those who are
developing OP2, and it is assumed they have already read the OP2 
Notes document.  Some of the material in that may be transferred
to this document in the future.

Those who are only using OP2 should instead read the User's Manual.  
\end{abstract}

\section{Plan construction}

This section deals with the algorithms in routine {\tt \bf plan} 
within the file {\tt \bf op\_lib.cu}.  This is called whenever a
parallel loop has at least one indirect dataset.

\subsection{indirect datasets}

``Sets'' are things like nodes or edges, a collection of abstract 
``elements'' over which the parallel loops execute.
``Datasets'' are the data associated with the sets, such as flow 
variables or edge weights, which are the arguments to the parallel 
loop functions.

In a particular parallel loop, an ``indirect dataset'' is one which 
is referenced indirectly using a mapping from another set.  Note that 
more than one argument of the parallel loop can address the same 
indirect dataset.  For example, in a typical CFD edge flux calculation, 
two arguments will correspond to the flow variables belonging to the 
nodes at either end of the edge, and another two arguments will 
correspond to the flux residuals at either end.

The pre-processor {\tt \bf op2.m} identifies for each parallel loop
the number of arguments {\tt nargs}, the number of indirect datasets
{\tt ninds} and the mapping from arguments to indirect datasets 
{\tt inds[]}.   The last of these has an entry for each argument; 
if it is equal to $-1$ then the argument does not reference an 
indirect dataset.
All of this information is supplied as input to the routine 
{\tt \bf plan}.

\subsection{local renumbering}

The execution plan divides the execution set into mini-partitions.
These are referred to in the code as ``blocks'' because it's a 
shorter word. This is a slightly different use of the word ``block'' 
compared to CUDA thread blocks, but each plan block is worked on by 
a single CUDA block so it's hopefully not too confusing.

The plan blocks are sized so that the indirect datasets will fit 
within the limited amount of shared memory available to each SM 
(``streaming multiprocessor'', NVIDIA's preferred term to describe 
each of the execution units in their GPUs).  The idea is that the
indirect datasets are held within the shared memory to maximize data 
reuse and avoid global memory traffic.  However this requires 
renumbering of the mappings used to reference these datasets.

%\newpage

For each plan block, and each indirect dataset within it, the 
algorithm for the renumbering is:
\begin{itemize}
\item
build a list of all references to the dataset by simply appending 
to a list
\item
sort the list and eliminate duplicates -- this then defines the 
mapping from local indices to global indices
\item
use a large work array to invert the mapping, to give the mapping 
from global indices to local indices (note: this is obviously only
needed for the global indices occurring within that block)
\item
create a new copy of the mapping table which uses the new local 
indices
\end{itemize}

Note that each indirect dataset ends up with its own duplicate 
mapping table.  In some cases, the indirect datasets had the same
original mapping tables; for example, in the CFD edge flux loop 
described before the flow variables and flux residuals were 
referenced using the same edge-node mappings.  In this case,
we are currently wasting both memory and memory bandwidth by 
duplicating the renumbered mapping tables.  This should be 
eliminated in the future, by identifying such duplication, 
de-allocating the duplicates, and changing the pointer to the 
duplicate table to point to the primary table.

Note also that for multi-dimensional mappings one has to use the
appropriate mapping index as specified in the inputs, and the 
re-numbered mappings which are stored are for that index alone.

\newpage

\subsection{coloring}

Coloring is used at two levels to avoid data conflicts.  The 
elements within each block are colored, and then the blocks 
themselves are colored.

We start by describing the element coloring. The goal is 
to assign a color to each element of that no two elements
of the same color reference the same indirect dataset element.

Conceptually, for each indirect dataset element we maintain
a list of the colors of the elements which reference it. 
Starting with this list initialised to be empty, the mathematical
algorithm treats each set element in turn and performs the 
following steps:
\begin{itemize}
\item 
loop over all indirect dataset elements referenced by the set
element to find the lowest index color which does not already 
reference them
\item
set this to be the color of the element
\item
loop again over all indirect datasets, adding this color
to their list
\end{itemize}

The efficient implementation of this uses bit operations.
The color list for each indirect dataset element is a 
32-bit integer in a work array, with the $i^{th}$ bit set 
to 1 if it is referenced by an element of color $i$.
The first step is performed by using the bit-wise {\tt or}
operation to combine the color lists into a variable called
{\tt mask}, followed by using the {\tt ffs} instruction to 
find the first zero bit.  The third step is also performed 
by a bit-wise {\tt or} operation.

Doing it in this way, we can process up to 32 colors in a 
single pass.  This is probably sufficient for most applications, 
but when it is not, the code loops back. i.e.~in the first pass,
if the first step finds that all bits are already set, it doesn't 
assign a color to the element, and goes on to the next element.
Then at the end it goes back to process the elements which have 
not yet been colored, with the color lists re-initialised to
indicate that the indirect set elements are not referenced by 
any of the ``new'' colors.  This outer loop (controlled by the 
variable {\tt repeat}) is repeated until all elements have been 
colored.

The block coloring is performed in exactly the same way, except 
that in the first and third steps the loop is over all indirect
dataset elements referenced by all of the elements in the block, 
not just by a single element.

\newpage

\subsection{block mapping}

The final part of {\tt \bf plan} defines a block mapping.
As illustrated in the bottom row of Fig.~\ref{fig:blkmap}, 
{\tt \bf plan} constructs blocks and stores their data in 
the order in which they are generated, so they are not 
grouped by color.

Rather than reordering the blocks to group them by color, 
I instead construct the {\tt blkmap} mapping from a grouped 
arrangement to the actual blocks.  This, together with the 
number of blocks of each color, is all that is needed for 
the later kernel execution.

The algorithm to compute {\tt blkmap} is:
\begin{itemize}
\item
compute the total number of blocks of each color
\item
do a cumulative summation to obtain the sum
of all blocks of preceding colors
\item
processing each block in turn, add the number of preceding
blocks of the same color to the cumulative sum of
preceding colors, to obtain its position in the {\tt blkmap} 
array
\item
finally, undo the cumulative summation operation to store 
the number of blocks of each color
\end{itemize}

\begin{figure}
\begin{center}
{\setlength{\unitlength}{0.25in}\begin{picture}(20,5.5)

\multiput(0,4)(1,0){20}{\framebox(1,1){}}
\multiput(0,0)(1,0){20}{\framebox(1,1){}}

\multiput( 0,4)(1,0){5}{\framebox(1,1){0}}
\multiput( 5,4)(1,0){5}{\framebox(1,1){1}}
\multiput(10,4)(1,0){5}{\framebox(1,1){2}}
\multiput(15,4)(1,0){5}{\framebox(1,1){3}}

\multiput( 0,0)(4,0){5}{\framebox(1,1){0}}
\multiput( 1,0)(4,0){5}{\framebox(1,1){1}}
\multiput( 2,0)(4,0){5}{\framebox(1,1){2}}
\multiput( 3,0)(4,0){5}{\framebox(1,1){3}}

\put(0.5,4){\vector(0,-1){3}}
\multiput(4.5,1)(4,0){4}{\vector(0,-1){0}}
\multiput(1.5,1)(4,0){3}{\vector(0,-1){0}}

\put(3,1){\oval(3,2)[tr]}
\put(4,1){\oval(9,3)[tr]}
\put(5,1){\oval(15,4)[tr]}
\put(6,1){\oval(21,5)[tr]}

\put(3,4){\oval(3,4)[bl]}
\put(4,4){\oval(3,3)[bl]}
\put(5,4){\oval(3,2)[bl]}
\put(6,4){\oval(3,1)[bl]}

\put(3,1){\oval(3,1)[tl]}
\put(3,4){\oval(5,5)[br]}

\put(6,1){\oval(1,1)[tl]}
\put(6,4){\oval(1,5)[br]}

\put(8,1){\oval(3,1)[tr]}
\put(8,4){\oval(1,5)[bl]}

\put(-0.5,0.5){\makebox(0,0)[r]{blocks}}
\put(-0.5,4.5){\makebox(0,0)[r]{blkmap}}

\end{picture}}
\end{center}

\caption{Illustration of block mapping, with colors indicated as 0, 1, etc.}
\label{fig:blkmap}
\end{figure}


\subsection{rest}

The first part of {\tt \bf plan} checks whether there is an existing plan 
to deal with this parallel loop, and if not it does some self-consistency 
checking.

The final part of {\tt \bf plan} computes the maximum amount of shared 
memory required by any of the blocks, and copies the plan arrays over 
onto the GPU, keeping the pointers in the {\tt plan} structure.


\subsection{op\_plan struct}

The first part of the {\tt \bf op\_plan} struct stores the input arguments
used to construct the plan. These are needed to determine whether the inputs 
for a new parallel loop match an existing plan.

The second part contains the data generated by the {\tt \bf op\_plan} routine:
\begin{itemize}
\item {\tt nthrcol${}^*$}:
an array with the number of thread colors for each block
\item {\tt thrcol${}^*$}:
an array with the thread color for each element of the primary set
\item {\tt offset${}^*$}:
an array with the primary set offset for the beginning of each block
\item {\tt ind\_maps${}^*$}:
a 2D array, the outer index over the indirect datasets, and the inner one giving the 
local $\rightarrow$ global renumbering for the elements of the indirect set
\item {\tt ind\_offs${}^*$}:
a 2D array, the outer index over the indirect datasets, and the inner one giving the 
starting offset into {\tt ind\_maps} for each block
\item {\tt ind\_sizes${}^*$}:
a 2D array, the outer index over the indirect datasets, and the inner one giving the
number of indirect elements for each block
\item {\tt maps${}^*$}:
a 2D array, the outer index over the datasets, and the inner one giving the indirect 
mappings to local indices in shared memory
\item {\tt nelems${}^*$}:
an array with number of primary set elements in each block
\item {\tt ncolors}:
number of block colors
\item {\tt ncolblk}:
an array with number of blocks for each color
\item {\tt blkmap${}^*$}:
an array with mapping to blocks of each color
\item {\tt nshare}:
number of bytes of shared memory require to execute the plan
\end{itemize}

The data in the arrays marked ${}^*$ is initially generated on the host, then 
transferred to the device, with only the array pointers being retained on the host.

\subsection{op\_plan\_check}

This routine checks the correctness of various aspects of a plan.  The checks 
are performed automatically after the plan is constructed, depending on the
value of the diagnostics variable {\tt OP\_DIAGS}.


\newpage

\section{op2.m preprocessor}

In this section I describe the code transformation performed by
{\bf op2.m} and discuss various aspects of the code which is generated.

\subsection{parsing the op\_par\_loop calls}

As explained in Section 1.1, the pre-processor finds each 
{\tt op\_par\_loop} call 
and parses the arguments to identify the number of indirect 
datasets which are used, and to define the {\tt inds[]} mapping 
from the arguments to the indirect datasets.  It also identifies 
how each of the arguments is being used (or ``accessed'').

If there are no indirect datasets the stub and kernel functions
which are generated are fairly simple.  The descriptions in the next 
two sections are for the more interesting case in which there is
at least one indirect dataset.

\subsection{the stub routine}

The stub routine is the host routine which is called by the user's 
main code.  If there are any local constants or global reduction 
operations it starts by transferring this data to the GPU.

It then calls {\tt \bf plan} to generate the execution plan, 
passing into it the information about indirect datasets which 
has been determined by the parser.

It then calls the kernel function to execute the plan.  This is 
done within a loop over the different blocks colors, with an
implicit synchronization between each color to avoid any data 
conflicts

One of the kernel parameters is the amount of dynamic shared 
memory; this comes from the maximum requirement determined 
by {\tt \bf plan}.

Finally, for global reductions it fetches the output data back
to the CPU.

\newpage

\subsection{the CUDA kernel routine}

Most of the code in {\bf op2.m} is for the generation of the CUDA 
kernel routines.  To understand this it is probably best to look
at an example of the generated code (e.g.~{\tt res\_kernel.cu})
while reading this description.

The key bits of code which are generated do the following:
\begin{itemize}
\item
declare correct number and type of input arguments, including
indirect datasets

\item
declare working variables, including local arrays which will probably
be held in registers (or in L1 cache on Fermi)

\item
get block ID using {\tt blkmap} mapping discussed in Section 1.4

\item
set the dynamic shared memory pointers for indirect datasets; see 
the CUDA Programmer's Guide for more info on this

\item
copy the read-only indirect datasets into shared memory, 
and zero out the memory for those being incremented

\item
synchronize to ensure all shared memory data is ready before proceeding

\item
loop over all set elements in the block, and for each one
 \begin{itemize}
 \item zero out the local arrays for those being incremented
 \item execute the user function
 \item use thread coloring to increment the shared memory data,
       with thread synchronization after each color
 \end{itemize}

\item
increment global storage of indirect datasets 

\item
complete any global reductions by updating the values in the main device 
memory

\end{itemize}

\newpage

Note: it is likely that the compiler will put small local arrays of 
known size into registers.  This is why users should specify
{\tt op\_par\_loop} array dimensions as a literal constant.
(Currently, {\bf op2.m} doesn't handle dimensions which are set at 
run-time, but that capability should be added in the future.)

There's one technical implementation detail which is very confusing.
In the code, the number of elements in the block is {\tt nelems}.
The variable {\tt nelems2} is equal to this value rounded up to the 
nearest multiple of the number of threads in the thread block.
This ensures that every thread goes through the main loop the same 
number of times.  This is important because the thread synchronization 
command {\tt \_\_syncthreads();} must be called by all threads.
The {\tt if} test within the loop prevents execution for elements
beyond the end of the block, and the default color is set to ensure
no participation in the colored increment.
%If we switch to atomic updates, the thread synchronization will
%no longer be needed, and so the main loop can simply go up to
%{\tt nelems}.

The generated code includes a number of pointers which are computed 
by thread 0 and held in shared memory.  This is because the same values 
are needed for all threads, and this minimises register usage.


\subsection{the new source file}

The new source file generated by {\bf op2.m} has only minor changes 
from the original source file:
\begin{itemize}
\item
new header file and declaration of function prototypes

\item
new names for each {\tt op\_par\_loop} call
\end{itemize}

\subsection{the master kernels file}

The master kernels file is a single file which includes the kernel 
files for each parallel loop along with some header files and the 
declaration of the global constant variables which come from parsing 
any {\tt op\_decl\_const} calls.

It has to be done this way in the current version of CUDA for the 
constants to have global scope over all kernel files.

\newpage

\section{Global reductions}

\subsection{Summation}

Each thread block has a separate entry in a device GPU array which is 
initialised to zero.

Each thread sums its contributions, with the sum being initialised to zero.
The combined contributions from a single thread block are then combined 
in a binary tree reduction process modelled on that the SDK ``reduction'' 
example, using shared memory.  

The thread block sum is then added to the appropriate global entry for that 
block.  Once the CUDA kernel call is complete the final block values are 
transferred back to the CPU and added to the original starting value.

\subsection{Min/max reductions}

The treatment of min/max reductions is very similar. Each thread block 
again has a separate entry in a device GPU array, but in this case 
it is initialised to the initial CPU input value.

At the thread level the minimum/maximum is initialised using the current 
global value for that block, and then updated using the thread's subsequent 
contributions.  A binary tree approach combines these to form a thread 
block minimum/maximum.

The thread block minimum/maximum values are used to update the global value.
Once the CUDA kernel call is complete the final block values are transferred 
back to the CPU and combined to give the overall minimum/maximum.

%\newpage

\section{User-defined datatypes}

OP2 supports user-defined datatypes for datasets, global constants, local constants 
and global reductions.

\subsection{Run-time type-checking}

%I considered using {\tt type\_id()} to check that user-defined types 
%match the type string declared by the user.  However, it appears this 
%is very compiler-dependent and not portable.

Run-time type-checking is implemented in a portable way in which the user 
is required to provide a {\tt type\_error} routine for each user-defined 
datatype.  The {\tt type\_error} routines for the standard datatypes are 
defined in {\tt op\_datatypes.h}.  

\subsection{Zero element for incrementing}

When executing {\tt op\_par\_loop} for a case in which a dataset is 
incremented, the CUDA code which is generated by {\bf op2.m} 
initialises the increment to zero, calls the user-supplied kernel
function to compute/add the new increment, and then uses coloring to 
add the increment to the dataset.

With a user-defined datatype, the addition in the last step requires
that the user has defined an overloaded addition operator.  The user
also has to specify a zero element, called {\tt ZERO\_typename},
which is used in the first step to initialise the increment correctly.

The zero elements for the standard datatypes are defined in 
{\tt op\_datatypes.h}.  Note that {\tt typedef} is used to alias 
{\tt unsigned int}, {\tt long long} and {\tt unsigned long long} to 
{\tt uint}, {\tt ll} and {\tt ull}, respectively.  This is because 
{\tt typename} must not contain any spaces, otherwise there are problems
with the compiler parsing of {\tt ZERO\_typename}.

\subsection{Global reductions}

Summations should be fine, and min/max reductions also ought to work 
provided the user has correctly overloaded the inequality operators 
so that for all $a, b, c$, 
\begin{itemize}
\item\vspace*{-0.1in} $a\!>\!b\ \cap\ b\!>\!c\ \Longrightarrow\ a\!>\!c$ 
           ~~ and ~~   $a\!<\!b\ \cap\ b\!<\!c\ \Longrightarrow\ a\!<\!c$ 
\item\vspace*{-0.1in}  either $a\!<\!b$, or $a\!>\!b$, or $a\!=\!b$
\end{itemize}


\subsection{Alignment padding}

When user-defined datatypes have elements with different sizes, the 
compiler automatically introduces padding to ensure correct data 
alignment, plus padding at the end, if necessary, to ensure that 
the next element in an array starts with the correct alignment.

However, when using dynamic shared memory in CUDA, it is the 
programmer's responsibility to ensure the correct alignment, as 
explained in section B.2.3 of the CUDA Programming Guide, version 3.0.
This requires the use of a {\tt ROUND\_UP} macro in {\bf op\_lib.cu}
and the CUDA kernels generated by {\bf op2.m}.  This macro is
modelled on the macro {\tt ALIGN\_UP} in the Programming Guide.

Similar padding is used in assembling local constants and global
reduction values into contiguous arrays for transfer to/from the GPU. 

\subsection{Future MPI treatment}

Looking to the future, MPI messaging will be performed by treating all 
datasets as having elements of type {\tt MPI\_CHAR} with the appropriate
byte size.  The only minor issue with this is the inability to use 
standard MPI reduction operators, but this could be handled by
implementing an all-to-all data exchange followed by local reduction,
with an option for MPI reductions being used for standard datatypes.


%\newpage

\section{CUDA 3.0 notes: 20/02/10}

Having read the new CUDA 3.0 Programmer's Guide and the guide on
Tuning CUDA Applications for Fermi, here I make a few notes which 
I should read over again in the future.

\begin{itemize}
\item
should think about turning off L1 caching of global data so L1 cache is 
preserved for register spilling of local variables

\item
should use the {\tt restrict} qualifier to enable compiler optimization

\item
new LDU instruction will be good for local constants; must make sure to use
{\tt const} qualifier

\item
use 48kB shared memory for kernels with indrect datasets, and
48kB L1 cache for those without

\item
L2 cache has the equivalent of 48kB per SM, as much as the maximum 
shared memory, so maybe it would be good to use it?  But on the other 
hand, the effective size is smaller (as it also stores data which is 
not wanted) and we're unlikely to be able to achieve a perfect 
allocation of data to shared memory and cached global memory, so 
maybe it's not worth worrying about?

\item
no point in using texture mapping?

\end{itemize}

\newpage

\section{To do list}

\begin{itemize}
\item
Modify {\bf op2.m} to cope with more than one parallel loop using 
the same user kernel function, and more than one call to set the 
value of the same global constant.

\item
Add capability to handle dataset (and local and global constant) dimensions 
which are not known until run-time.

\item
Add recursive geometric partitioning for local renumbering of sets and 
mappings to improve data reuse.

\item
For some parallel loops (like the gradient calculation in HYDRA)
which are data-intensive and not compute-intensive, it might be better 
for the thread coloring to also control the user kernel execution, so 
that the indirect arrays held in shared memory can be directly incremented 
rather than using local variables to hold the temporary increments.

\item
Within a thread block, could reorder elements to reduce the number of 
different colors within each warp.  This would require the storage and 
fetching of the permuted identity mapping 

\end{itemize}

\end{document}


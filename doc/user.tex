\documentclass[11pt]{article}

\usepackage[colorlinks,urlcolor=blue]{hyperref}


 \topmargin 0.in  \headheight 0pt  \headsep 0pt  \raggedbottom
 \oddsidemargin 0.1in 
 \textheight 9.5in  \textwidth 6.00in 
 \parskip 5pt plus 1pt minus 1pt
 \def \baselinestretch {1.25}   % one-and-a-half spaced
 \setlength {\unitlength} {0.75in}
%
%
\newenvironment{routine}[2]
{\vspace{.25in}{\noindent\bf\hspace{0pt} #1}{\\ \noindent #2}
\begin{list}{}{
\renewcommand{\makelabel}[1]{{\tt ##1} \hfil} 
\itemsep 0pt plus 1pt minus 1pt
\leftmargin  1.2in
\rightmargin 0.0in
\labelwidth  1.1in
\itemindent  0.0in
\listparindent  0.0in
\labelsep    0.05in}
}{\end{list}}
%

\begin{document}

\title{OP2 User's Manual (phase 1)}
\author{Mike Giles}
\maketitle


\newpage
\section{Introduction}

OP2 is an API with associated libraries and preprocessors to 
generate parallel executables for applications on unstructured grids.  
The initial API is for C, but FORTRAN 77 will also be supported.

The key concept behind OP2 is that unstructured grids can be described
by a number of sets.  Depending on the application, these sets might
be of nodes, edges, triangular faces, quadrilateral faces, cells of
a variety of types, far-field boundary nodes, wall boundary faces, etc.
Associated with these sets are both data (e.g.~coordinate data at 
nodes) and mappings to other sets (e.g.~edge mapping to the two 
nodes at each end of the edge).  All of the numerically-intensive
operations can then be described as a loop over all members of a set, 
carrying out some operations on data associated directly
with the set or with another set through a mapping.   

OP2 makes the important restriction that the order in which the
function is applied to the members of the set must not affect the
final result.  This allows the parallel implementation to choose
its own ordering to achieve maximum parallel efficiency. 
Two other restrictions are that the sets and maps are static 
(i.e.~they do not change) and the operands in the set operations 
are not referenced through a double level of mapping indirection 
(i.e.~through a mapping to another set which in turn uses another 
mapping to data in a third set).

OP2 currently enables users to write a single program which can be 
built into three different executables for different platforms:
\begin{itemize}
\item
single-threaded on a CPU
\item
parallelised using CUDA for NVIDIA GPUs
\item
multi-threaded using OpenMP for multicore x86 systems
\end{itemize}

In the longer-term there will be support for AVX vectorisation for
x86 CPUs, and OpenCL for both CPUs and GPUS.

There will also be support for distributed-memory MPI 
parallelisation in combination with any of the above.  This will 
require parallel file I/O and so there will be routines to handle 
file I/O for the main datasets, as well as routines to handle 
terminal I/O.

\newpage
\section{Overview}

A computational project can be viewed as involving three steps:
\begin{itemize}
\item
writing the program
\item
debugging the program, often using a small testcase
\item
running the program on increasingly large applications
\end{itemize}

With OP2 we want to simplify the first two tasks, while 
providing as much performance as possible for the third one.  

To achieve the high performance for large applications, a 
preprocessor is needed to generate the CUDA code for GPUs 
or OpenMP code for multicore x86 systems.  However, to keep 
the initial development simple, the single-threaded executable 
does not use any special tools; the user's main code is simply 
linked to a set of library routines, most of which do little 
more than error-checking to assist the debugging process by 
checking the correctness of the user's program.  Note that this 
single-threaded version will not execute efficiently.  The
preprocessor is needed to generate efficient OpenMP code for 
x86 systems.

Figure \ref{fig:seq} shows the build process for a single 
thread CPU executable.  The user's main program (in this case 
{\tt jac.cpp}) uses the OP header file {\tt op\_seq.h} and is 
linked to the OP routines in {\tt op\_seq.c} using {\tt g++},
perhaps controlled by a Makefile.

Figure \ref{fig:cuda} shows the build process for the corresponding
CUDA executable.  The preprocessor parses the user's main program 
and produces a modified main program and a CUDA file which
includes a separate file for each of the kernel functions.  These 
are then compiled and linked to the OP routines in {\tt op\_lib.cu} 
using {\tt g++} and the NVIDIA CUDA compiler {\tt nvcc}, again 
perhaps controlled by a Makefile.
As well as the header file {\tt op\_seq.h} which is included by
the user's main code {\tt jac.cpp}, there is a header file
{\tt op\_datatypes.h} which is included by all of the files
in the CUDA implementation, and by {\tt op\_seq.h}.

Figure \ref{fig:op} shows the OpenMP build process which is very 
similar to the CUDA process except that it uses {\tt *.cpp} files
produced by the preprocessor instead of {\tt *.cu} files.

In looking at the API specification, users may think it is
a little verbose in places. e.g.~users have to re-supply 
information about the datatype of the datasets being used
in a parallel loop.  This is a deliberate choice to simplify
the task of the preprocessor, and therefore hopefully reduce
the chance for errors.  It is also motivated by the thought that
{\bf ``programming is easy; it's debugging which is difficult''}.
i.e.~writing code isn't time-consuming, it's correcting it
which takes the time.  Therefore, it's not unreasonable to ask
the programmer to supply redundant information, but be assured 
that the preprocessor or library will check that all redundant
information is self-consistent.  If you declare a dataset as being 
of type {\tt OP\_DOUBLE} and later say that it is of type 
{\tt OP\_FLOAT} this will be flagged up as an error at run-time, 
both in the single-threaded library and in the CUDA library.

\newpage

\begin{figure}
\begin{center}
{\setlength{\unitlength}{1in}
\begin{picture}(4.5,2)

\put(-0.2,1.6){\framebox(0.8,0.3){\tt op\_seq.h}}
\put(1,1.5){\framebox(1,0.5){\tt jac.cpp}}

\put(0.65,1.75){\line(1,0){0.1}}
\put(0.85,1.75){\vector(1,0){0.1}}

\put(2.5,1.5){\framebox(1,0.5){\tt op\_seq.c}}

\put(1.5,1.5){\vector(0,-1){0.625}}

\put(3,1.5){\vector(0,-1){0.625}}

\put(2.25,0.5){\oval(2.5,0.75)}
\put(2.25,0.5){\makebox(0,0){make / g++}}

\end{picture}}
\end{center}

\caption{Sequential code build process}
\label{fig:seq}
\end{figure}



\begin{figure}
\begin{center}
{\setlength{\unitlength}{1in}
\begin{picture}(7,5)

\put(1.5,4){\framebox(1,0.5){\tt jac.cpp}}

\put(2.0,4.0){\vector(0,-1){0.625}}

\put(2.1,3.0){\oval(4.2,0.75)}
\put(2.1,3.0){\makebox(0,0){op2.m preprocessor}}

\put(0.5,2.625){\vector(0,-1){0.625}}
\put(2.0,2.625){\vector(0,-1){0.625}}
\put(3.5,2.625){\vector(0,-1){0.625}}

\put(0.0,1.5){\framebox(1,0.5){\tt jac\_op.cpp}}
\put(1.3,1.5){\framebox(1.2,0.5){\tt jac\_kernels.cu}}
\put(2.8,1.5){\framebox(1.4,0.5){}}
\put(3.5,1.85){\makebox(0,0){\tt res\_kernel.cu}}
\put(3.5,1.65){\makebox(0,0){\tt update\_kernel.cu}}
\put(4.5,1.5){\framebox(1,0.5){\tt op\_lib.cu}}

\put(0.5,1.5){\vector(0,-1){0.625}}
\put(2.0,1.5){\vector(0,-1){0.625}}
\put(5.0,1.5){\vector(0,-1){0.625}}

\put(2.62,1.75){\vector(-1,0){0.1}}
\put(2.78,1.75){\line(-1,0){0.1}}


\put(2.75,0.5){\oval(5.5,0.75)}
\put(2.75,0.5){\makebox(0,0){make / nvcc / g++}}

\end{picture}}
\end{center}

\caption{CUDA code build process}
\label{fig:cuda}
\end{figure}



\begin{figure}
\begin{center}
{\setlength{\unitlength}{1in}
\begin{picture}(7,5)

\put(1.5,4){\framebox(1,0.5){\tt jac.cpp}}

\put(2.0,4.0){\vector(0,-1){0.625}}

\put(2.1,3.0){\oval(4.2,0.75)}
\put(2.1,3.0){\makebox(0,0){op2.m preprocessor}}

\put(0.5,2.625){\vector(0,-1){0.625}}
\put(2.0,2.625){\vector(0,-1){0.625}}
\put(3.5,2.625){\vector(0,-1){0.625}}

\put(0.0,1.5){\framebox(1,0.5){\tt jac\_op.cpp}}
\put(1.3,1.5){\framebox(1.2,0.5){\tt jac\_kernels.cpp}}
\put(2.8,1.5){\framebox(1.4,0.5){}}
\put(3.5,1.85){\makebox(0,0){\tt res\_kernel.cpp}}
\put(3.5,1.65){\makebox(0,0){\tt update\_kernel.cpp}}
\put(4.5,1.5){\framebox(1,0.5){\tt op\_lib.cpp}}

\put(0.5,1.5){\vector(0,-1){0.625}}
\put(2.0,1.5){\vector(0,-1){0.625}}
\put(5.0,1.5){\vector(0,-1){0.625}}

\put(2.62,1.75){\vector(-1,0){0.1}}
\put(2.78,1.75){\line(-1,0){0.1}}


\put(2.75,0.5){\oval(5.5,0.75)}
\put(2.75,0.5){\makebox(0,0){make / icc}}

\end{picture}}
\end{center}

\caption{OpenMP code build process}
\label{fig:op}
\end{figure}


\clearpage

\newpage
\section{Initialisation and termination routines}

\begin{routine} {op\_init(int argc, char **argv)}
{This routine must be called before all other OP routines.}
\item \vspace{-0.4in}
\end{routine}

\begin{routine} {op\_decl\_set(int size, op\_set *set, char *name)}
{This routine declares information about a set.}

\item[size]          number of elements in the set
\item[set]           output OP set ID
\item[name]          a name used for output diagnostics
\end{routine}

\begin{routine} {op\_decl\_map(op\_set from, op\_set to, int dim, int *imap, op\_map *map, char *name)}
{This routine declares information about a mapping from one set to another.}

\item[from]          set pointed from
\item[to]            set pointed to
\item[dim]           number of mappings per element
\item[imap]          input mapping table
\item[map]           output OP mapping ID
\item[name]          a name used for output diagnostics
\end{routine}

\begin{routine} {op\_decl\_const(int dim, char *type, T *dat, char *name)}
{This routine declares constant data with global scope to be used in user's kernel functions.
Note: in sequential version, it is the user's responsibility to define the appropriate global
variable.}

\item[dim]           dimension of data (i.e.~array size)

                     at present this must be a literal constant (i.e. a number not a variable);
                     this restriction will be removed in the future but a literal constant will
                     remain more efficient
\item[type]          datatype, either intrinsic (``float'', ``double'', ``int'', ``uint'',
                     ``ll'', ``ull'' or ``bool'') or user-defined
\item[dat]          input data of type {\tt T}  (checked for consistency with {\tt type} at run-time)
\item[name]          global name to be used in user's kernel functions;\\
                     a scalar variable if {\tt dim=1}, otherwise an array of size {\tt dim}
\end{routine}


\newpage

\begin{routine} {op\_decl\_dat(op\_set set, int dim, char *type, T *dat, op\_dat *data,\\ char *name)}
{This routine declares information about data associated with a set.}

\item[set]           set
\item[dim]           dimension of dataset (number of items per set element)

                     at present this must be a literal constant (i.e. a number not a variable);
                     this restriction will be removed in the future but a literal constant will
                     remain more efficient
\item[type]          datatype, either intrinsic or user-defined
\item[dat]          input data of type {\tt T}  (checked for consistency with {\tt type} at run-time)
\item[dat]           output OP dataset ID
\item[name]          a name used for output diagnostics
\end{routine}

\begin{routine} {op\_fetch\_data(op\_dat dat)}
{This routine transfers data from the GPU back to the CPU.}
\item[dat]           OP dataset ID -- data is put back into original input array 
\end{routine}


\begin{routine} {op\_diagnostic\_output()}
{This routine prints out various useful bits of diagnostic info about sets, mappings and datasets}
\item \vspace{-0.3in}
\end{routine}


\begin{routine} {op\_exit()}
{This routine must be called last to cleanly terminate the OP computation.}
\item \vspace{-0.3in}
\end{routine}



\newpage
\section{Parallel execution routine}

As an example, the parallel loop syntax when the user's kernel function has 3 arguments,
with the third being a local constant or global reduction array, is:

\begin{routine} {op\_par\_loop\_3(void (*kernel)(T0 *, T1 *, T2 *), char *name, op\_set set,\\
\hspace*{0.13in}    op\_dat arg0, int idx0, op\_map map0, int dim0, char *typ0, op\_access acc0,\\
\hspace*{0.13in}    op\_dat arg1, int idx1, op\_map map1, int dim1, char *typ1, op\_access acc1,\\
\hspace*{0.13in}    T2     *arg2, int idx2, op\_map map2, int dim2, char *typ2, op\_access acc2)}{}

\item[kernel]     user's kernel function with 3 arguments of arbitrary type\\
                  (this is only used for the single-threaded CPU build)
\item[name]       name of kernel function, used for output diagnostics
\item[set]        OP set ID, giving set over which the parallel computation is performed
\item[arg]        OP dataset ID, or pointer to constant or global reduction array
\item[idx]        index of mapping to be used (-1 $\equiv$ no mapping indirection)
\item[map]        OP mapping ID ({\tt OP\_ID} for identity mapping, i.e.~no mapping indirection,
                  {\tt OP\_GBL} for constant or global reduction array)
\item[dim]        dataset dimension (redundant info, checked at run-time for consistency)

                  at present this must be a literal constant (i.e. a number not a variable);
                  this restriction will be removed in the future but a literal constant will
                  remain more efficient
\item[typ]        dataset datatype (redundant info, checked at run-time for consistency)
\item[acc]        access type:\\
                  {\tt OP\_READ}: read-only\\ 
                  {\tt OP\_WRITE}: write-only, but without potential data conflict\\ 
                  {\tt OP\_RW}:  read and write, but without potential data conflict\\
                  {\tt OP\_INC}: increment, or global reduction to compute a sum\\ 
                  {\tt OP\_MAX}: global reduction to compute a maximum \\ 
                  {\tt OP\_MIN}: global reduction to compute a minimum \\ 
\end{routine}

\noindent
In this example, {\tt kernel} is a function with 3 arguments of arbitrary type which
performs a calculation for a single set element.
This will get converted by a preprocessor into a routine called by the CUDA kernel 
function.  The preprocessor will also take the specification of the arguments and turn 
this into the CUDA kernel function which loads in indirect data (i.e.~data addressed 
indirectly through a mapping) from the device main memory into the shared storage, 
then calls the converted {\tt kernel} function for each element for each line in 
the above specification.  Indirect data is incremented in shared memory (with
thread coloring to avoid possible data conflicts) before being updated at the end 
of the CUDA kernel call.

The restriction that {\tt OP\_WRITE} and {\tt OP\_RW} access must not have any 
potential data conflict means that two different elements of the set cannot 
through a mapping indirection reference the same elements of the dataset.  

%(If they did, it would be undetermined which final value would be assigned to
%the dataset element.  Users might argue that it would be OK if both assigned
%the same value, but I don't want to handle that complication.)

Furthermore, with {\tt OP\_WRITE} the user's kernel function must set the 
value of all {\tt DIM} components of the dataset.  If the user's kernel function
does not set all of them, the access should be specified to be {\tt OP\_RW}
since the kernel function needs to read in the old values of the components
which are not being modified. 

Different numbers of arguments are handled similarly by routines with names
of the form {\tt op\_par\_loop\_n} where {\tt n} is the number of arguments.
Each argument can be either a dataset or a local constant or global reduction 
array, following the syntax shown above.


\section{User-defined datatypes}

If the user defines a new datatype {\tt mytype} then this must be included in
a header file along with
\begin{itemize}
\item
a type-checking routine:

{\tt 
inline int type\_error(const mytype *,const char *type)\\
\{return strcmp(type,"mytype");\}
}

which is used at run-time to check the consistency of the user's type declarations
in input arguments.

\item

a ``zero element'' declaration of the form:

{\tt \#define ZERO\_mytype    0;}

as well as an appropriate overloaded addition operator  if there is any 
{\tt OP\_INC} access to the datatype.  The zero element and overloaded 
addition have to be such that $0 + x = x$ where $x$ represents any element 
of the user's datatype and $0$ represents the declared zero element.

\item

an overloaded implementation of the inequality operators $<$ and $>$ 
if there are any {\tt OP\_MIN, OP\_MAX} accesses to the datatype.

\end{itemize}

In addition, the user must specify the name of the new header file using the 
environment variable {\tt OP\_USER\_DATATYPES}
so that this header file is included into the OP2 header file {\tt op\_datatypes.h}.


\newpage


\section{Preprocessor}

The prototype preprocessor has been written in MATLAB.  It is run by the command

{\tt op2('main')}

\noindent
where {\tt main.cpp} is the user's main program.  It produces as output a 
modified main program {\tt main\_op.cpp}, and a new CUDA file {\tt main\_kernels.cu}
which includes one or more files of the form {\tt xxx\_kernel.cu} containing 
the CUDA implementations of the user's kernel functions.

If the user's application is split over several files it is run by a command such as

{\tt op2('main','sub1','sub2','sub3')}

\noindent
where {\tt sub1.cpp, sub2.cpp, sub3.cpp} are the additional input files which will 
lead to the generation of output files {\tt sub1\_op.cpp, sub2\_op.cpp, sub3\_op.cpp} 
in addition to {\tt main\_op.cpp, main\_kernels.cu} and the individual kernel files.

The preprocessor cannot currently handle cases in which the same user kernel is 
used in more than one parallel loop, or when global constant data is set/updated 
in more than one place within the code.  This will be addressed in the future.


\section{Future changes}

There will be a new function {\tt \bf op\_partition} which will re-number 
all of the elements in each set, to maximise the data reuse within each 
mini-partition.  This is likely to use the same partitioning algorithm which
will be employed for the higher-level distributed-memory partitioning for the
MPI implementation in phase 2.

\newpage

\section{Phase 2 proposal}

As explained in the introduction, phase 2 of the OP2 project will handle 
distributed-memory parallelisation using MPI.  Because this links into other
work by Leigh Lapworth and others at Rolls-Royce, discussions have
begun about how this will be handled within OP2, and this has led to the 
following proposal.

My starting point is that we anticipate dealing with extremely large datasets
and so we need to support parallel file I/O.  There also seems to be
general agreement that 
\href{http://www.hdfgroup.org/HDF5/}{HDF5}
has become the {\it de facto} standard underlying file format, 
with various other standards like
\href{http://cgns.sourceforge.net/hdf5.html}{CGNS}
layered on top.

Originally, my idea was to modify the OP2 set, mapping and dataset declarations
so that these were read in by OP2 from a specified HDF5 file using specified 
keywords.  Thus the OP2 library would have been entirely responsible for the 
parallel file I/O.

However, my new proposal is to adopt a layered approach:
\begin{itemize}
\item
a minor extension to the existing API, leaving the parallel file I/O to the 
developer

\item
an example implementation of the parallel file I/O for HDF files, which some
developers may choose to use unaltered, and others may modify to suit their needs
\end{itemize}

The rationale for this is to allow developers to make the tradeoff between 
ease-of-use and flexibility.  Some will want maximum ease-of-use and are prepared 
to pay the price of working with HDF5 files with the flat keyword-based hierarchy
which we will assume.  Others will want the flexibility 
to manage their data storage in the way they wish, and will accept the additional
programming effort this will entail.

In an MPI application, multiple copies of the same program are executed as
separate processes, often on different nodes of a compute cluster.  Hence, the 
OP2 declarations will be invoked on each process.  The extensions to the existing 
API are as follows:
\begin{itemize}
\item {\bf op\_decl\_set}: {\tt size} is the number of elements of the set which
will be provided by this MPI process

\item {\bf op\_decl\_map}: {\tt imap} provides the part of the mapping table 
which corresponds to its share of the {\tt from} set

\item {\bf op\_decl\_dat}: {\tt dat} provides the data which corresponds to its 
share of {\tt set}
\end{itemize}

For example, if an application has 4 processes, $4\!\times\! 10^6$ nodes and 
$16 \!\times\! 10^6$ edges, then each process might be responsible for providing
$10^6$ nodes and $4\!\times\! 10^6$ edges. Process 0 (the one with MPI rank 0)
would be responsible for providing the first $10^6$ nodes, process 1 the 
next $10^6$ nodes, and so on, and the same for the edges.

The edge $\rightarrow$ node mapping tables would still contain the same 
information as in a single process implementation, but process 0 would provide
the first $4\!\times\! 10^6$ entries, process 1 the next $4\!\times\! 10^6$ entries, 
and so on.

This is effectively using a simple contiguous block partitioning of the datasets, 
but it is very important to note that this will not be used for the parallel 
computation.  OP2 will re-partition the datasets (in parallel, probably using 
\href{http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview}{\tt parmetis}), 
will re-number the mapping tables as needed (as well as constructing import/export 
lists for halo data exchange) and will move all data/mappings/datasets to the 
correct MPI process.

The second layer would look similar to the existing API:
\begin{itemize}
\item {\bf op\_decl\_set\_hdf5}: similar to {\bf op\_decl\_set} but with {\tt size} 
replaced by {\tt file} which defines the HDF5 file from which {\tt size} 
is read using keyword {\tt name}

\item {\bf op\_decl\_map\_hdf5}: similar to {\bf op\_decl\_map} but with {\tt imap} 
replaced by {\tt file} from which the mapping table is read using keyword {\tt name}

\item {\bf op\_decl\_dat\_hdf5}: similar to {\bf op\_decl\_dat} but with {\tt dat} 
replaced by {\tt file} from which the data is read using keyword {\tt name}
\end{itemize}

\end{document}




